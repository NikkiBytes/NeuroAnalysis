{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding with ANOVA + SVM: Chocolate Data -- imagine data (full session)\n",
    "### This is a simple decoding using feature selection followed by an SVM.  \n",
    "\n",
    "Reference: http://nilearn.github.io/auto_examples/02_decoding/plot_haxby_anova_svm.html#sphx-glr-auto-examples-02-decoding-plot-haxby-anova-svm-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this imports all the commands needed for the script to work#\n",
    "import os\n",
    "import numpy as np\n",
    "import nilearn\n",
    "import glob\n",
    "#import matplotlib\n",
    "import nibabel as nib\n",
    "import pandas as pd \n",
    "from nilearn.input_data import NiftiMasker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ana=os.path.join('/projects/niblab/nilearn_projects/CS_avg_mprage_image.nii.gz')\n",
    "#image mask\n",
    "imag_mask=os.path.join('/projects/niblab/nilearn_projects/power_roimask_4bi.nii.gz')\n",
    "#our behavioral csv file \n",
    "stim = os.path.join('/projects','niblab','scripts','nilean_stuff','label_all_sub.csv')\n",
    "#our dataset concatenated image \n",
    "dataset='/projects/niblab/bids_projects/Experiments/ChocoData/derivatives/group_ana/w1_imagine_all.nii.gz'\n",
    "\n",
    "#load behavioral data into a pandas df\n",
    "behavioral = pd.read_csv(stim, sep=\",\")\n",
    "#grab conditional labels \n",
    "conditions = behavioral[\"label\"]\n",
    "\n",
    "#restrict data to our target analysis - app vs h2O\n",
    "condition_mask = behavioral[\"label\"].isin([\"app\", \"H2O\"])\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "#confirm we have the # of condtions needed\n",
    "print(conditions.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['H2O' 'app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record these as an array of sessions, with fields for condition and run \n",
    "session = behavioral[condition_mask].to_records(index=False)\n",
    "print(session.dtype.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('labels', 'subs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for decoding, standardizing the data is often very important  \n",
    "# if data hasn't been smoothed, you can do that here too  \n",
    "masker = NiftiMasker(mask_img=imag_mask, standardize=True, memory=\"nilearn_cache\", memory_level=1)\n",
    "X = masker.fit_transform(dataset)\n",
    "#Apply our condition mask\n",
    "X=X[condition_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function to be used. Here we use a SVC with a linear kernel \n",
    "# Here we are using a 'rbf' kernel, and we set the decision function shape to 'ovr', standing for 'one vs rest'\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "\n",
    "# Now we define the dimension reduction to be used. \n",
    "# Here we use a classical univariate feature selection based on F-test, namely Anova.\n",
    "# When doing full brain analysis, it's best to use SelectPercentile, keeping 5% of the voxels \n",
    "# (because it is independent of the resolution of the data )\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif \n",
    "feature_selection = SelectPercentile(f_classif, percentile=5)\n",
    "\n",
    "\n",
    "# We now plug our classifier and our feature selection into a pipeline that performs 2 operations successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the decoder and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_svc.fit(X, conditions)\n",
    "y_pred = anova_svc.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the prediction scores through Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "\n",
    "# Define the cross validation scheme used for validation. \n",
    "# Here we use the LeaveOneGroupOut CV on the session group, which corresponds to a Leave-One-session-out\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "# Compute the prediction accuracy for the different folds(i.e session)\n",
    "cv_scores = cross_val_score(anova_svc, X, conditions, cv=cv, groups=session)\n",
    "\n",
    "#Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the results:\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "      (classification_accuracy, 1. / len(conditions.unique())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy: 0.0029 / Chance level: 0.500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = svc.coef_\n",
    "# reverse feature selection\n",
    "coef = feature_selection.inverse_transform(coef)\n",
    "# reverse masking\n",
    "weight_img = masker.inverse_transform(coef)\n",
    "\n",
    "\n",
    "# Use the mean image as a background to avoid relying on anatomical data\n",
    "from nilearn import image\n",
    "mean_img = image.mean_img(func_filename)\n",
    "\n",
    "# Create the figure\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "plot_stat_map(weight_img, mean_img, title='SVM weights')\n",
    "\n",
    "# Saving the results as a Nifti file may also be important\n",
    "weight_img.to_filename('haxby_face_vs_house.nii')\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load behavioral data into a pandas df\n",
    "behavioral = pd.read_csv(stim, sep=\",\")\n",
    "#grab conditional labels \n",
    "conditions = behavioral[\"label\"]\n",
    "\n",
    "#restrict data to our target analysis - app vs h2O\n",
    "condition_mask = behavioral[\"label\"].isin([\"unapp\", \"app\", \"H2O\"])\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "#confirm we have the # of condtions needed\n",
    "print(conditions.unique())\n",
    "# Record these as an array of sessions, with fields for condition and run \n",
    "session = behavioral[condition_mask].to_records(index=False)\n",
    "print(session.dtype.names)\n",
    "# for decoding, standardizing the data is often very important  \n",
    "# if data hasn't been smoothed, you can do that here too  \n",
    "masker = NiftiMasker(mask_img=imag_mask, standardize=True, memory=\"nilearn_cache\", memory_level=1)\n",
    "X = masker.fit_transform(dataset)\n",
    "#Apply our condition mask\n",
    "X=X[condition_mask]\n",
    "# Define the prediction function to be used. Here we use a SVC with a linear kernel \n",
    "# Here we are using a 'rbf' kernel, and we set the decision function shape to 'ovr', standing for 'one vs rest'\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf', decision_function_shape='ovr')\n",
    "\n",
    "# Now we define the dimension reduction to be used. \n",
    "# Here we use a classical univariate feature selection based on F-test, namely Anova.\n",
    "# When doing full brain analysis, it's best to use SelectPercentile, keeping 5% of the voxels \n",
    "# (because it is independent of the resolution of the data )\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif \n",
    "feature_selection = SelectPercentile(f_classif, percentile=5)\n",
    "\n",
    "\n",
    "# We now plug our classifier and our feature selection into a pipeline that performs 2 operations successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])\n",
    "\n",
    "anova_svc.fit(X, conditions)\n",
    "y_pred = anova_svc.predict(X)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "\n",
    "# Define the cross validation scheme used for validation. \n",
    "# Here we use the LeaveOneGroupOut CV on the session group, which corresponds to a Leave-One-session-out\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "# Compute the prediction accuracy for the different folds(i.e session)\n",
    "cv_scores = cross_val_score(anova_svc, X, conditions, cv=cv, groups=session)\n",
    "\n",
    "#Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the results:\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "      (classification_accuracy, 1. / len(conditions.unique())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy: 0.0002 / Chance level: 0.333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load behavioral data into a pandas df\n",
    "behavioral = pd.read_csv(stim, sep=\",\")\n",
    "#grab conditional labels \n",
    "conditions = behavioral[\"label\"]\n",
    "\n",
    "#restrict data to our target analysis - app vs h2O\n",
    "condition_mask = behavioral[\"label\"].isin([\"unapp\", \"app\", \"H2O\"])\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "#confirm we have the # of condtions needed\n",
    "print(conditions.unique())\n",
    "# Record these as an array of sessions, with fields for condition and run \n",
    "session = behavioral[condition_mask].to_records(index=False)\n",
    "print(session.dtype.names)\n",
    "# for decoding, standardizing the data is often very important  \n",
    "# if data hasn't been smoothed, you can do that here too  \n",
    "masker = NiftiMasker(mask_img=imag_mask, standardize=True, memory=\"nilearn_cache\", memory_level=1)\n",
    "X = masker.fit_transform(dataset)\n",
    "#Apply our condition mask\n",
    "X=X[condition_mask]\n",
    "# Define the prediction function to be used. Here we use a SVC with a linear kernel \n",
    "# Here we are using a 'rbf' kernel, and we set the decision function shape to 'ovr', standing for 'one vs rest'\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Now we define the dimension reduction to be used. \n",
    "# Here we use a classical univariate feature selection based on F-test, namely Anova.\n",
    "# When doing full brain analysis, it's best to use SelectPercentile, keeping 5% of the voxels \n",
    "# (because it is independent of the resolution of the data )\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif \n",
    "feature_selection = SelectPercentile(f_classif, percentile=5)\n",
    "\n",
    "\n",
    "# We now plug our classifier and our feature selection into a pipeline that performs 2 operations successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])\n",
    "\n",
    "anova_svc.fit(X, conditions)\n",
    "y_pred = anova_svc.predict(X)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "\n",
    "# Define the cross validation scheme used for validation. \n",
    "# Here we use the LeaveOneGroupOut CV on the session group, which corresponds to a Leave-One-session-out\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "# Compute the prediction accuracy for the different folds(i.e session)\n",
    "cv_scores = cross_val_score(anova_svc, X, conditions, cv=cv, groups=session)\n",
    "\n",
    "#Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the results:\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "      (classification_accuracy, 1. / len(conditions.unique())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy: 0.0002 / Chance level: 0.333333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load behavioral data into a pandas df\n",
    "behavioral = pd.read_csv(stim, sep=\",\")\n",
    "#grab conditional labels \n",
    "conditions = behavioral[\"label\"]\n",
    "\n",
    "#restrict data to our target analysis - app vs h2O\n",
    "condition_mask = behavioral[\"label\"].isin(\"app\", \"H2O\"])\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "#confirm we have the # of condtions needed\n",
    "print(conditions.unique())\n",
    "# Record these as an array of sessions, with fields for condition and run \n",
    "session = behavioral[condition_mask].to_records(index=False)\n",
    "print(session.dtype.names)\n",
    "# for decoding, standardizing the data is often very important  \n",
    "# if data hasn't been smoothed, you can do that here too  \n",
    "masker = NiftiMasker(mask_img=imag_mask, standardize=True, memory=\"nilearn_cache\", memory_level=1)\n",
    "X = masker.fit_transform(dataset)\n",
    "#Apply our condition mask\n",
    "X=X[condition_mask]\n",
    "# Define the prediction function to be used. Here we use a SVC with a linear kernel \n",
    "# Here we are using a 'rbf' kernel, and we set the decision function shape to 'ovr', standing for 'one vs rest'\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Now we define the dimension reduction to be used. \n",
    "# Here we use a classical univariate feature selection based on F-test, namely Anova.\n",
    "# When doing full brain analysis, it's best to use SelectPercentile, keeping 5% of the voxels \n",
    "# (because it is independent of the resolution of the data )\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif \n",
    "feature_selection = SelectPercentile(f_classif, percentile=5)\n",
    "\n",
    "\n",
    "# We now plug our classifier and our feature selection into a pipeline that performs 2 operations successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])\n",
    "\n",
    "anova_svc.fit(X, conditions)\n",
    "y_pred = anova_svc.predict(X)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "\n",
    "# Define the cross validation scheme used for validation. \n",
    "# Here we use the LeaveOneGroupOut CV on the session group, which corresponds to a Leave-One-session-out\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "# Compute the prediction accuracy for the different folds(i.e session)\n",
    "cv_scores = cross_val_score(anova_svc, X, conditions, cv=cv, groups=session)\n",
    "\n",
    "#Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the results:\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "      (classification_accuracy, 1. / len(conditions.unique())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy: 0.1089 / Chance level: 0.500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using SVM with Anova and a selectkpercentile we are getting terrible accuracy results. We tried with both a RBF and linear kernel.  \n",
    "  \n",
    "    \n",
    "This next process we increase the percentile to 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load behavioral data into a pandas df\n",
    "behavioral = pd.read_csv(stim, sep=\",\")\n",
    "#grab conditional labels \n",
    "conditions = behavioral[\"label\"]\n",
    "\n",
    "#restrict data to our target analysis - app vs h2O\n",
    "condition_mask = behavioral[\"label\"].isin(\"app\", \"H2O\"])\n",
    "conditions = conditions[condition_mask]\n",
    "\n",
    "#confirm we have the # of condtions needed\n",
    "print(conditions.unique())\n",
    "# Record these as an array of sessions, with fields for condition and run \n",
    "session = behavioral[condition_mask].to_records(index=False)\n",
    "print(session.dtype.names)\n",
    "# for decoding, standardizing the data is often very important  \n",
    "# if data hasn't been smoothed, you can do that here too  \n",
    "masker = NiftiMasker(mask_img=imag_mask, standardize=True, memory=\"nilearn_cache\", memory_level=1)\n",
    "X = masker.fit_transform(dataset)\n",
    "#Apply our condition mask\n",
    "X=X[condition_mask]\n",
    "# Define the prediction function to be used. Here we use a SVC with a linear kernel \n",
    "# Here we are using a 'rbf' kernel, and we set the decision function shape to 'ovr', standing for 'one vs rest'\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Now we define the dimension reduction to be used. \n",
    "# Here we use a classical univariate feature selection based on F-test, namely Anova.\n",
    "# When doing full brain analysis, it's best to use SelectPercentile, keeping 5% of the voxels \n",
    "# (because it is independent of the resolution of the data )\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif \n",
    "feature_selection = SelectPercentile(f_classif, percentile=5)\n",
    "\n",
    "\n",
    "# We now plug our classifier and our feature selection into a pipeline that performs 2 operations successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', svc)])\n",
    "\n",
    "anova_svc.fit(X, conditions)\n",
    "y_pred = anova_svc.predict(X)\n",
    "\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "\n",
    "# Define the cross validation scheme used for validation. \n",
    "# Here we use the LeaveOneGroupOut CV on the session group, which corresponds to a Leave-One-session-out\n",
    "cv = LeaveOneGroupOut()\n",
    "\n",
    "# Compute the prediction accuracy for the different folds(i.e session)\n",
    "cv_scores = cross_val_score(anova_svc, X, conditions, cv=cv, groups=session)\n",
    "\n",
    "#Return the corresponding mean prediction accuracy\n",
    "classification_accuracy = cv_scores.mean()\n",
    "\n",
    "# print the results:\n",
    "print(\"Classification accuracy: %.4f / Chance level: %f\" %\n",
    "      (classification_accuracy, 1. / len(conditions.unique())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
